<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Christopher Schymura | publications</title>
  <meta name="description" content="Personal website of Christopher Schymura.">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Christopher</span> Schymura</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    curriculum vitae
                    
                  </a>
              </li>
            
          
            
          
            
          
            
          
            
              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/talks/">
                    talks
                    
                  </a>
              </li>
            
          
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>publications</h1>
  <h6>peer-reviewed journal and conference papers</h6>



<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://konvens.org/site/" target="_blank">
          KONVENS
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="DSK2021a" class="col p-0">
      <h5 class="title mb-0">Data Science Kitchen at GermEval 2021: A Fine Selection of Hand-Picked Features, Delivered Fresh from the Oven.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr>Niclas Hildebrandt,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="https://www.ruhr-uni-bochum.de/ika/mitarbeiter/boenninghoff.htm" target="_blank">Benedikt Bönninghoff</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Dennis Orth,</nobr>
                
              
            
          
        
          
            
              and
              
                <nobr><em>Christopher Schymura</em>.</nobr>
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Konferenz zur Verarbeitung natürlicher Sprache/Conference on Natural Language Processing (KONVENS)
          
          
            2021.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#DSK2021a-abstract" role="button" aria-expanded="false" aria-controls="DSK2021a-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://netlibrary.aau.at/obvukloa/content/pageview/6435304" target="_blank">PDF</a>
        
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="http://arxiv.org/abs/2109.02383" target="_blank">arXiv</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="DSK2021a-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            This paper presents the contribution of the Data Science Kitchen at GermEval 2021 shared task on the identification of toxic, engaging, and fact-claiming comments. The task aims at extending the identification of offensive language, by including additional subtasks that identify comments which should be prioritized for fact-checking by moderators and community managers. Our contribution focuses on a feature-engineering approach with a conventional classification backend. We combine semantic and writing style embeddings derived from pre-trained deep neural networks with additional numerical features, specifically designed for this task. Ensembles of Logistic Regression classifiers and Support Vector Machines are used to derive predictions for each subtask via a majority voting scheme. Our best submission achieved macro-averaged F1-scores of 66.8%, 69.9% and 72.5% for the identification of toxic, engaging, and fact-claiming comments.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://www.isca-speech.org" target="_blank">
          INTERSPEECH
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2021" class="col p-0">
      <h5 class="title mb-0">PILOT: Introducing Transformers for Probabilistic Sound Event Localization.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr><a href="https://www.ruhr-uni-bochum.de/ika/mitarbeiter/boenninghoff.htm" target="_blank">Benedikt Bönninghoff</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Tsubasa Ochiai,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/member/marcd/" target="_blank">Marc Delcroix</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/kinoshita/research.html" target="_blank">Keisuke Kinoshita</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/nakatani/" target="_blank">Tomohiro Nakatani</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/araki/index_e.htm" target="_blank">Shoko Araki</a>,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Annual Conference of the International Speech Communication Association (INTERSPEECH)
          
          
            2021.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2021-abstract" role="button" aria-expanded="false" aria-controls="Schymura2021-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/schymura21_interspeech.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="http://arxiv.org/abs/2106.03903" target="_blank">arXiv</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2021-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Sound event localization aims at estimating the positions of sound sources in the environment with respect to an acoustic receiver (e.g. a microphone array). Recent advances in this domain most prominently focused on utilizing deep recurrent neural networks. Inspired by the success of transformer architectures as a suitable alternative to classical recurrent neural networks, this paper introduces a novel transformer-based sound event localization framework, where temporal dependencies in the received multi-channel audio signals are captured via self-attention mechanisms. Additionally, the estimated sound event positions are represented as multivariate Gaussian variables, yielding an additional notion of uncertainty, which many previously proposed deep learning-based systems designed for this application do not provide. The framework is evaluated on three publicly available multi-source sound event localization datasets and compared against state-of-the-art methods in terms of localization error and event detection accuracy. It outperforms all competing systems on all datasets with statistical significant differences in performance.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://ieeexplore.ieee.org/xpl/conhome/1000002/all-proceedings" target="_blank">
          ICASSP
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Wissing2021" class="col p-0">
      <h5 class="title mb-0">Data Fusion for Audiovisual Speaker Localization: Extending Dynamic Stream Weights to the Spatial Domain.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr><a href="https://de.linkedin.com/in/julio-wissing" target="_blank">Julio Wissing</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="https://www.ruhr-uni-bochum.de/ika/mitarbeiter/boenninghoff.htm" target="_blank">Benedikt Bönninghoff</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Tsubasa Ochiai,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/member/marcd/" target="_blank">Marc Delcroix</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/kinoshita/research.html" target="_blank">Keisuke Kinoshita</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/nakatani/" target="_blank">Tomohiro Nakatani</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/araki/index_e.htm" target="_blank">Shoko Araki</a>,</nobr>
                
              
            
          
        
          
            
              and
              
                <nobr><em>Christopher Schymura</em>.</nobr>
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
          
          
            2021.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Wissing2021-abstract" role="button" aria-expanded="false" aria-controls="Wissing2021-abstract">Abstract</a>
        
        
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/rub-ksv/spatial-stream-weights" target="_blank">Code</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="http://arxiv.org/abs/2102.11588" target="_blank">arXiv</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Wissing2021-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Estimating the positions of multiple speakers can be helpful for tasks like automatic speech recognition or speaker diarization. Both applications benefit from a known speaker position when, for instance, applying beamforming or assigning unique speaker identities. Recently, several approaches utilizing acoustic signals augmented with visual data have been proposed for this task. However, both the acoustic and the visual modality may be corrupted in specific spatial regions, for instance due to poor lighting conditions or to the presence of background noise. This paper proposes a novel audiovisual data fusion framework for speaker localization by assigning individual dynamic stream weights to specific regions in the localization space. This fusion is achieved via a neural network, which combines the predictions of individual audio and video trackers based on their time- and location-dependent reliability. A performance evaluation using audiovisual recordings yields promising results, with the proposed fusion approach outperforming all baseline models.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2021</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://ieeexplore.ieee.org/xpl/conhome/1801907/all-proceedings" target="_blank">
          EUSIPCO
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2020c" class="col p-0">
      <h5 class="title mb-0">Exploiting Attention-based Sequence-to-Sequence Architectures for Sound Event Localization.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Tsubasa Ochiai,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/member/marcd/" target="_blank">Marc Delcroix</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/kinoshita/research.html" target="_blank">Keisuke Kinoshita</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/nakatani/" target="_blank">Tomohiro Nakatani</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/araki/index_e.htm" target="_blank">Shoko Araki</a>,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In European Signal Processing Conference (EUSIPCO)
          
          
            2020.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2020c-abstract" role="button" aria-expanded="false" aria-controls="Schymura2020c-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9287224" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/chrschy/adrenaline" target="_blank">Code</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="http://arxiv.org/abs/2103.00417" target="_blank">arXiv</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2020c-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Sound event localization frameworks based on deep neural networks have shown increased robustness with respect to reverberation and noise in comparison to classical parametric approaches. In particular, recurrent architectures that incorporate temporal context into the estimation process seem to be well-suited for this task. This paper proposes a novel approach to sound event localization by utilizing an attention-based sequence-to-sequence model. These types of models have been successfully applied to problems in natural language processing and automatic speech recognition. In this work, a multi-channel audio signal is encoded to a latent representation, which is subsequently decoded to a sequence of estimated directions-of-arrival. Herein, attentions allow for capturing temporal dependencies in the audio signal by focusing on specific frames that are relevant for estimating the activity and direction-of-arrival of sound events at the current time-step. The framework is evaluated on three publicly available datasets for sound event localization. It yields superior localization performance compared to state-of-the-art methods in both anechoic and reverberant conditions.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://www.ijcnn.org/" target="_blank">
          IJCNN
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Freiwald2020" class="col p-0">
      <h5 class="title mb-0">Loss Functions for Deep Monaural Speech Enhancement.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr>Jan Freiwald,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Lea Schönherr,</nobr>
                
              
            
          
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Steffen Zeiler,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In International Joint Conference on Neural Networks (IJCNN)
          
          
            2020.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Freiwald2020-abstract" role="button" aria-expanded="false" aria-controls="Freiwald2020-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="http://vigir.missouri.edu/ gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-21276.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Freiwald2020-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Deep neural networks have proven highly effective at speech enhancement, which makes them attractive not just as front-ends for machine listening and speech recognition, but also as enhancement models for the benefit of human listeners. They are, however, usually being trained on loss functions that only assess quality in terms of a minimum mean squared error. This is neglecting the fact that human audio perception functions in a manner far better described by logarithmic measures than linear ones, that psychoacoustic hearing thresholds limit the perceptibility of many signal components in a mixture, and that a degree of continuity of signals may also be expected. Hence, sudden changes in the gain of a system may be detrimental. In the following, we cast these properties of human perception into a form that can aid the optimization of a deep neural network speech enhancement system. We explore their effects on a range of model topologies, showing the efficacy of the proposed modifications.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://ieeexplore.ieee.org/xpl/conhome/1000002/all-proceedings" target="_blank">
          ICASSP
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2020b" class="col p-0">
      <h5 class="title mb-0">A Dynamic Stream Weight Backprop Kalman Filter for Audiovisual Speaker Tracking.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Tsubasa Ochiai,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/member/marcd/" target="_blank">Marc Delcroix</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/kinoshita/research.html" target="_blank">Keisuke Kinoshita</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/nakatani/" target="_blank">Tomohiro Nakatani</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/araki/index_e.htm" target="_blank">Shoko Araki</a>,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
          
          
            2020.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2020b-abstract" role="button" aria-expanded="false" aria-controls="Schymura2020b-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9054005" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2020b-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Audiovisual speaker tracking is an application that has been tackled by a wide range of classical approaches based on Gaussian filters, most notably the well-known Kalman filter. Recently, a specific Kalman filter implementation was proposed for this task, which incorporated dynamic stream weights to explicitly control the influence of acoustic and visual observations during estimation. Inspired by recent progress in the context of integrating uncertainty estimates into modern deep learning frameworks, this paper proposes a deep neural-network-based implementation of the Kalman filter with dynamic stream weights, whose parameters can be learned via standard backpropagation. This allows for jointly optimizing the parameters of the model and the dynamic stream weight estimator in a unified framework. An experimental study on audiovisual speaker tracking shows that the proposed model shows comparable performance to state-of-the-art recurrent neural networks with the additional advantage of requiring a smaller number of parameters and providing explicit uncertainty information.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing" target="_blank">
          TASLP
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2020a" class="col p-0">
      <h5 class="title mb-0">Audiovisual Speaker Tracking using Nonlinear Dynamical Systems with Dynamic Stream Weights.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In IEEE/ACM Transactions on Audio, Speech, and Language Processing
          
          
            2020.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2020a-abstract" role="button" aria-expanded="false" aria-controls="Schymura2020a-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9037104" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2020a-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Data fusion plays an important role in many technical applications that require efficient processing of multimodal sensory observations. A prominent example is audiovisual signal processing, which has gained increasing attention in automatic speech recognition, speaker localization and related tasks. If appropriately combined with acoustic information, additional visual cues can help to improve the performance in these applications, especially under adverse acoustic conditions. A dynamic weighting of acoustic and visual streams based on instantaneous sensor reliability measures is an efficient approach to data fusion in this context. This article presents a framework that extends the well-established theory of nonlinear dynamical systems with the notion of dynamic stream weights for an arbitrary number of sensory observations. It comprises a recursive state estimator based on the Gaussian filtering paradigm, which incorporates dynamic stream weights into a framework closely related to the extended Kalman filter. Additionally, a convex optimization approach to estimate oracle dynamic stream weights in fully observed dynamical systems utilizing a Dirichlet prior is presented. This serves as a basis for a generic parameter learning framework of dynamic stream weight estimators. The proposed system is application-independent and can be easily adapted to specific tasks and requirements. A study using audiovisual speaker tracking tasks is considered as an exemplary application in this work. An improved tracking performance of the dynamic stream-weight-based estimation framework over state-of-the-art methods is demonstrated in the experiments.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing" target="_blank">
          TASLP
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Trowitzsch2020" class="col p-0">
      <h5 class="title mb-0">Joining Sound Event Detection and Localization Through Spatial Segregation.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr>Ivo Trowitzsch,</nobr>
                
              
            
          
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://www.eecs.tu-berlin.de/menue/einrichtungen/professuren/professorinnen/obermayer" target="_blank">Klaus Obermayer</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In IEEE/ACM Transactions on Audio, Speech, and Language Processing
          
          
            2020.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Trowitzsch2020-abstract" role="button" aria-expanded="false" aria-controls="Trowitzsch2020-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/8928942" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Trowitzsch2020-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Identification and localization of sounds are both integral parts of computational auditory scene analysis. Although each can be solved separately, the goal of forming coherent auditory objects and achieving a comprehensive spatial scene understanding suggests pursuing a joint solution of the two problems. This article presents an approach that robustly binds localization with the detection of sound events in a binaural robotic system. Both tasks are joined through the use of spatial stream segregation which produces probabilistic time-frequency masks for individual sources attributable to separate locations, enabling segregated sound event detection operating on these streams. We use simulations of a comprehensive suite of test scenes with multiple co-occurring sound sources, and propose performance measures for systematic investigation of the impact of scene complexity on this segregated detection of sound types. Analyzing the effect of spatial scene arrangement, we show how a robot could facilitate high performance through optimal head rotation. Furthermore, we investigate the performance of segregated detection given possible localization error as well as error in the estimation of number of active sources. Our analysis demonstrates that the proposed approach is an effective method to obtain joint sound event location and type information under a wide range of conditions.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2020</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://ieeexplore.ieee.org/xpl/conhome/1000002/all-proceedings" target="_blank">
          ICASSP
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2019" class="col p-0">
      <h5 class="title mb-0">Learning Dynamic Stream Weights for Linear Dynamical Systems Using Natural Evolution Strategies.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
          
          
            2019.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2019-abstract" role="button" aria-expanded="false" aria-controls="Schymura2019-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/8682249" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2019-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Multimodal data fusion is an important aspect of many object localization and tracking frameworks that rely on sensory observations from different sources. A prominent example is audiovisual speaker localization, where the incorporation of visual information has shown to benefit overall performance, especially in adverse acoustic conditions. Recently, the notion of dynamic stream weights as an efficient data fusion technique has been introduced into this field. Originally proposed in the context of audiovisual automatic speech recognition, dynamic stream weights allow for effective sensory-level data fusion on a per-frame basis, if reliability measures for the individual sensory streams are available. This study proposes a learning framework for dynamic stream weights based on natural evolution strategies, which does not require the explicit computation of oracle information. An experimental evaluation based on recorded audiovisual sequences shows that the proposed approach outperforms conventional methods based on supervised training in terms of localization performance.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2019</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://ieeexplore.ieee.org/xpl/conhome/1801907/all-proceedings" target="_blank">
          EUSIPCO
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Ito2018" class="col p-0">
      <h5 class="title mb-0">Noisy cGMM: Complex Gaussian Mixture Model with Non-Sparse Noise Model for Joint Source Separation and Denoising.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/ito/" target="_blank">Nobutaka Ito</a>,</nobr>
                
              
            
          
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/araki/index_e.htm" target="_blank">Shoko Araki</a>,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="http://www.kecl.ntt.co.jp/icl/signal/nakatani/" target="_blank">Tomohiro Nakatani</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In European Signal Processing Conference (EUSIPCO)
          
          
            2018.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Ito2018-abstract" role="button" aria-expanded="false" aria-controls="Ito2018-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/8553410" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Ito2018-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Here we introduce a noisy cGMM, a probabilistic model for noisy, mixed signals observed by a microphone array for joint source separation and denoising. In a conventional time-varying complex Gaussian mixture model (cGMM), the observed signals are assumed to be composed of sparse target signals only, where the sparseness refers to the property of having significant power at only a few time-frequency points. However, this assumption becomes inaccurate in the presence of non-sparse signals such as background noise, which renders speech enhancement based on the cGMM less effective. In contrast, the proposed noisy cGMM is based on the assumption that the observed signals consist of not only sparse target signals but also non-sparse background noise. This enables the noisy cGMM to model the observed signals accurately even in the presence of non-sparse background noise, which leads to effective speech enhancement. We also propose a joint diagonalization-based algorithm for estimating the model parameters of the noisy cGMM, which is significantly faster than the standard EM algorithm without any performance degradation. Indeed, the joint diagonalization bypasses the need for matrix inversion, matrix multiplication, and determinant computation at each time-frequency point, which are needed in the EM algorithm. In an experiment, the noisy cGMM outperformed the cGMM in joint source separation and denoising.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="http://www.iwaenc.org/" target="_blank">
          IWAENC
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2018c" class="col p-0">
      <h5 class="title mb-0">Extending Linear Dynamical Systems with Dynamic Stream Weights for Audiovisual Speaker Localization.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Tobias Isenberg,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In International Workshop on Acoustic Signal Enhancement (IWAENC)
          
          
            2018.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2018c-abstract" role="button" aria-expanded="false" aria-controls="Schymura2018c-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/8521384" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2018c-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            An important aspect of audiovisual speaker localization is the appropriate fusion of acoustic and visual observations based on their time-varying reliability. In this study, a framework which incorporates dynamic stream weights into the well-known Kalman filtering framework is proposed to cope with this challenge. The concept of dynamic stream weights has recently been investigated in the context of audiovisual automatic speech recognition, where it was successfully applied to weight audiovisual observations according to their reliability. This study extends that approach to linear dynamical systems and additionally introduces a closed-form solution to compute oracle dynamic stream weights from observation sequences with known state trajectories. The proposed approach is evaluated on audiovisual recordings from a humanoid robot in reverberant environments. The results indicate that incorporating dynamic stream weights allows for efficient data fusion on a per-frame basis, which shows superior performance over conventional Kalman-filter-based state estimation.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://link.springer.com/conference/ica" target="_blank">
          LVA/ICA
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2018b" class="col p-0">
      <h5 class="title mb-0">Exploiting Structures of Temporal Causality for Robust Speaker Localization in Reverberant Environments.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Peng Guo,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Yanir Maymon,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="https://sites.google.com/view/boazrafaely" target="_blank">Boaz Rafaely</a>,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Latent Variable Analysis and Signal Separation (LVA/ICA)
          
          
            2018.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2018b-abstract" role="button" aria-expanded="false" aria-controls="Schymura2018b-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://link.springer.com/chapter/10.1007/978-3-319-93764-9_22" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2018b-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            This paper introduces a framework for robust speaker localization in reverberant environments based on a causal analysis of the temporal relationship between direct sound and corresponding reflections. It extends previously proposed localization approaches for spherical microphone arrays based on a direct-path dominance test. So far, these methods are applied in the time-frequency domain without considering the temporal context of direction-of-arrival measurements. In this work, a causal analysis of the temporal structure of subsequent directions-of-arrival estimates based on the Granger causality test is proposed. The cause-effect relationship between estimated directions is modeled via a causal graph, which is used to distinguish the direction of the direct sound from corresponding reflections. An experimental evaluation in simulated acoustic environments shows that the proposed approach yields an improvement in localization performance especially in highly reverberant conditions.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://ieeexplore.ieee.org/xpl/conhome/1000002/all-proceedings" target="_blank">
          ICASSP
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2018a" class="col p-0">
      <h5 class="title mb-0">Potential-Field-Based Active Exploration for Acoustic Simultaneous Localization and Mapping.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
          
          
            2018.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2018a-abstract" role="button" aria-expanded="false" aria-controls="Schymura2018a-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/8461655" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2018a-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            This paper presents a novel framework for active exploration in the context of acoustic simultaneous localization and mapping (SLAM) using a microphone array mounted on a mobile robotic agent. Acoustic SLAM aims at building a map of acoustic sources present in the environment and simultaneously estimating the agent’s own trajectory and position within this map. Two important aspects of this task are robustness against disturbances arising from reverberation and sensor imperfections and an appropriate degree of exploration to achieve high map accuracy. Several approaches to the latter aspect using information-theoretic measures have recently been proposed. This study extends these approaches into a framework based on the potential field method, which is a widely used technique for robotic path planning and navigation. It allows to determine exploratory movement trajectories for the robotic agent via gradient descent, without requiring computationally expensive Monte Carlo simulations to predict the effects of specific trajectory choices. Furthermore, additional constraints like maintaining a safe distance to acoustic sources can easily be integrated into this framework. Experimental evaluation demonstrates that the proposed method yields adequate exploration strategies of the acoustic environment leading to accurate map estimates.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2018</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://ieeexplore.ieee.org/xpl/conhome/1000002/all-proceedings" target="_blank">
          ICASSP
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Meutzner0217" class="col p-0">
      <h5 class="title mb-0">Improving Audio-Visual Speech Recognition using Deep Neural Networks with Dynamic Stream Reliability Estimates.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr><a href="https://sites.google.com/site/hmeutz/" target="_blank">Hendrik Meutzner</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="http://staffwww.dcs.shef.ac.uk/people/N.Ma/" target="_blank">Ning Ma</a>,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="https://www.bucknell.edu/fac-staff/robert-nickel" target="_blank">Robert Nickel</a>,</nobr>
                
              
            
          
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
          
          
            2017.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Meutzner0217-abstract" role="button" aria-expanded="false" aria-controls="Meutzner0217-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/7953172" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Meutzner0217-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Audio-visual speech recognition is a promising approach to tackling the problem of reduced recognition rates under adverse acoustic conditions. However, finding an optimal mechanism for combining multi-modal information remains a challenging task. Various methods are applicable for integrating acoustic and visual information in Gaussian-mixture-model-based speech recognition, e.g., via dynamic stream weighting. The recent advances of deep neural network (DNN)-based speech recognition promise improved performance when using audio-visual information. However, the question of how to optimally integrate acoustic and visual information remains. In this paper, we propose a state-based integration scheme that uses dynamic stream weights in DNN-based audio-visual speech recognition. The dynamic weights are obtained from a time-variant reliability estimate that is derived from the audio signal. We show that this state-based integration is superior to early integration of multi-modal features, even if early integration also includes the proposed reliability estimate. Furthermore, the proposed adaptive mechanism is able to outperform a fixed weighting approach that exploits oracle knowledge of the true signal-to-noise ratio.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://ieeexplore.ieee.org/xpl/conhome/1000002/all-proceedings" target="_blank">
          ICASSP
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2017" class="col p-0">
      <h5 class="title mb-0">Monte Carlo Exploration for Active Binaural Localization.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Juan Diego Rios Grajales,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
          
          
            2017.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2017-abstract" role="button" aria-expanded="false" aria-controls="Schymura2017-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/7952204" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2017-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            This study introduces a machine hearing system for robot audition, which enables a robotic agent to pro-actively minimize the uncertainty of sound source location estimates through motion. The proposed system is based on an active exploration approach, providing a means to model and predict effects of the agent’s future motions on localization uncertainty in a probabilistic manner. Particle filtering is used to estimate the posterior probability density function of the source position from binaural measurements, enabling to jointly assess azimuth and distance of the source. The framework allows to infer and refine a policy to select appropriate actions via a Monte Carlo exploration approach. Experiments in simulated reverberant conditions are conducted, showing that active exploration and the incorporation of distance estimation significantly improve localization performance.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2017</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-2 p-0 abbr">
    
      
        <a class="badge font-weight-bold info-color-dark align-middle" style="width: 90px;" href="https://www.isca-speech.org" target="_blank">
          INTERSPEECH
        </a>
      
    
  </div>
  <div class="col-sm-10 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Schymura2015" class="col p-0">
      <h5 class="title mb-0">Binaural Sound Source Localisation and Tracking Using a Dynamic Spherical Head Model.</h5>
      <div class="author">
        
          
            
              
                <nobr><em>Christopher Schymura</em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Fiete Winter,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr><a href="https://etit.ruhr-uni-bochum.de/fakultaet/professuren/prof-dr-ing-dorothea-kolossa/" target="_blank">Dorothea Kolossa</a>,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr><a href="https://www.int.uni-rostock.de/Mitarbeiter-Info.23+B6Jkw9MCZjSGFzaD1hOTk4NjZjOGMxOWM0ZTliMjkwODU3YjUyNTM3NGFlYyZ0eF9qcHN0YWZmX3BpMSU1QnNob3dVaWQlNUQ9MTEx.0.html" target="_blank">Sascha Spors</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Annual Conference of the International Speech Communication Association (INTERSPEECH)
          
          
            2015.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Schymura2015-abstract" role="button" aria-expanded="false" aria-controls="Schymura2015-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.int.uni-rostock.de/fileadmin/user_upload/publications/spors/2015/Schymura_et_al_EURONOISE_Binaural_Localization.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Schymura2015-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            This paper introduces a binaural model for the localisation and tracking of a moving sound source’s azimuth in the horizontal plane. The model uses a nonlinear state space representation of the sound source dynamics including the current position of the listener’s head. The state is estimated via an unscented Kalman Filter by comparing the interaural level and time differences of the binaural signal with semi-analytically derived localisation cues from a spherical head model. The localisation performance of the model is evaluated in combination with two different head movement approaches based on open-and closed-loop control strategies. The results show that adaptive strategies outperform non-adaptive ones and are able to compensate systematic deviations between the spherical head model and human heads.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2015</h3>
    </div>
  </div>



  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2021 Christopher Schymura. § <a href="/imprint" title="Imprint">Imprint</a> | <a href="/privacy" title="Privacy">Privacy</a>
    <br />Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- GitHub Stars -->
  <script src="/assets/js/github-stars.js"></script>
  <script type="text/javascript">
    
  </script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-186875845-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
